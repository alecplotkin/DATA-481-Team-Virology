---
title: "Try2"
author: "Max Sleek"
date: "`r Sys.Date()`"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(reshape2)
library(e1071)
library(dplyr)
library(caret)
library(nnet)
library(keras)
library(NeuralNetTools)
library(lime)
```

# Make Read In Data Function
```{r}
read_to_matrix <- function(file_name) {
  return(as.matrix(read_csv(file_name))) 
}

data_20250318 <- read_to_matrix("KSV_Gex_Reduced.csv")
rownames(data_20250318) <- data_20250318[,1]
data_20250318 <- data_20250318[,-1]
correlation_20250318 <- read_to_matrix("KSV_Gex_Reduced_Correlation_Matrix.csv")
# May need to remove 4983 from the following 2 datasets
distances_20250318 <- read_to_matrix("gene_distance_matrix.csv")
clusterassignments_20250318 <- read_csv("Paper_Sample_Clusters.csv") %>%
  rename(sample_id = number)
```

# Make Heatmap Function
```{r}
heatmap <- function(M) {
  cor_D <- cor(M, use = "complete.obs")
melted_cor_D <- melt(cor_D)

ggplot(data = melted_cor_D, aes(x = Var1, y = Var2, fill = value)) +
  geom_tile() + 
  scale_fill_gradient2(low = 'blue', mid = 'white', high = 'red', midpoint = 0,limits = c(-1, 1))
}

heatmap(data_20250318)
```

# Make plots
```{r}
hist(data_20250318[1,])
plot(density(data_20250318[1,]), 
     main = "Density Plot", 
     xlab = "Value", 
     ylab = "Density", 
     col = "blue", 
     lwd = 2)
# For HW: Add rug to density plot
```

# Try matrix multiplication
```{r}
scalar <- rep(1, ncol(correlation_20250318) - 1)
as.numeric(correlation_20250318)*as.matrix(scalar)
```

# PCA
```{r}
scaled_data <- scale(data_20250318)
pca_result <- prcomp(scaled_data, center = TRUE, scale. = TRUE)
screeplot(pca_result, type = "lines", main = "Scree Plot")

#Biplot
biplot(pca_result, scale = 0, main = "PCA Biplot")

pca_df <- data.frame(pca_result$x[, 1:2])  # PC1 and PC2
pca_df$cluster <- as.factor(clusterassignments_20250318$cluster)

#Scatterplot of PCs
ggplot(pca_df, aes(x = PC1, y = PC2, color = cluster)) +
  geom_point(size = 2) +
  theme_minimal() +
  labs(title = "PCA: PC1 vs PC2 by Cluster")
```

# Make Plot of Expression vs. Gene Number
```{r}
matplot(data_20250318, type = 'l')
```

# Create mean vector, sort data ascending, make the same plot
```{r}
gene_means <- rowMeans(data_20250318)
matplot(gene_means, type = 'l')


sorted_data20250318 <- as.data.frame(cbind(gene_means, data_20250318))
sorted_data20250318 <- sorted_data20250318[order(sorted_data20250318$gene_means),]
matplot(sorted_data20250318, type = 'l', col = 'grey', lwd = 3)



# ggplot(sorted_data20250318, aes()) + geom_line()
```



# Try SVM
```{r}
# Convert rownames of data to numeric to match sample IDs
data_ids <- as.numeric(rownames(data_20250318))

# Combine data and cluster labels
data_df <- as.data.frame(data_20250318)
data_df$sample_id <- data_ids

# Merge data with cluster labels
merged_df <- inner_join(data_df, clusterassignments_20250318, by = "sample_id")
merged_df$cluster <- as.factor(merged_df$cluster)

# Split into features and labels
X <- merged_df %>% select(-sample_id, -cluster)
y <- merged_df$cluster

# Train/test split
set.seed(42)
train_idx <- createDataPartition(y, p = 0.7, list = FALSE)
X_train <- X[train_idx, ]
y_train <- y[train_idx]
X_test <- X[-train_idx, ]
y_test <- y[-train_idx]

# Train SVM model
svm_model <- svm(X_train, y_train, kernel = "linear", scale = TRUE)

# Predict on test set
predictions <- predict(svm_model, X_test)

# Evaluate performance
conf_mat <- confusionMatrix(predictions, y_test)
print(conf_mat)
```

# Plot Feature Coefficients
```{r}
# Extract SVM model weights (only works for linear kernel)
svm_weights <- t(svm_model$coefs) %*% svm_model$SV

# Create a named vector of absolute weights
importance <- abs(as.vector(svm_weights))
names(importance) <- colnames(X)

# Sort by importance
importance_sorted <- sort(importance, decreasing = TRUE)

# Plot top N features
top_n <- 20  # or however many you want to see
barplot(importance_sorted[1:top_n],
        las = 2,
        col = "skyblue",
        main = paste("Top", top_n, "Most Important Genes (Linear SVM)"),
        ylab = "Absolute Weight")
```

# Try Simple Neural Network
```{r}
# Normalize
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)

# Train neural network (1 hidden layer, 10 units)
nn_model <- nnet(X_train_scaled, class.ind(y_train), size = 10, softmax = TRUE, maxit = 200)

# Predict
pred_probs <- predict(nn_model, X_test_scaled, type = "raw")
pred_classes <- apply(pred_probs, 1, which.max)
true_classes <- as.numeric(y_test)

# Evaluate
conf_mat <- confusionMatrix(factor(pred_classes), factor(true_classes))
print(conf_mat)

# Plot feature importance
garson(nn_model)
```

# Try Keras NN
```{r}
use_virtualenv("r-tensorflow", required = TRUE)

# Use reticulate to be sure the binding is correct
reticulate::py_module_available("tensorflow")  # Should return TRUE
reticulate::py_module_available("keras")       # Should also return TRUE

# Scale features
X_train_scaled <- scale(X_train)
X_test_scaled <- scale(X_test)

# One-hot encode labels
my_to_categorical <- function(y) {
  y <- as.integer(y)
  n <- length(y)
  classes <- sort(unique(y))
  mat <- matrix(0, nrow = n, ncol = length(classes))
  for (i in seq_along(y)) {
    mat[i, y[i] + 1] <- 1  # +1 since R is 1-indexed
  }
  return(mat)
}

y_train_cat <- my_to_categorical(as.integer(y_train) - 1L)
y_test_cat  <- my_to_categorical(as.integer(y_test) - 1L)

# Build Model
model <- keras_model_sequential() %>%
  layer_dense(units = 64, activation = 'relu', input_shape = c(ncol(X_train_scaled))) %>%
  layer_dropout(rate = 0.3) %>%
  layer_dense(units = 32, activation = 'relu') %>%
  layer_dense(units = ncol(y_train_cat), activation = 'softmax')

model %>% compile(
  loss = 'categorical_crossentropy',
  optimizer = optimizer_adam(),
  metrics = 'accuracy'
)

history <- model %>% fit(
  X_train_scaled, y_train_cat,
  epochs = 30,
  batch_size = 16,
  validation_split = 0.2,
  verbose = 1
)
```

# Evaluate Keras
```{r}
scores <- model %>% evaluate(X_test_scaled, y_test_cat)
cat("Test accuracy:", scores$accuracy, "\n")

#Predictions
preds <- model %>% predict(X_test_scaled)
pred_classes <- max.col(preds)
true_classes <- as.numeric(y_test)

conf_mat <- confusionMatrix(factor(pred_classes), factor(true_classes))
print(conf_mat)

#Feature Importance
explainer <- lime(X_train_scaled, model, bin_continuous = FALSE)
explanation <- explain(X_test_scaled[1:10, ], explainer, n_features = 10)

plot_features(explanation)
```

